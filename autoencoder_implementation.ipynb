{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nn.nn import NeuralNetwork as nn\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "dataset = sklearn.datasets.load_digits()\n",
    "\n",
    "#extract features and labels\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "#normalize data\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "\n",
    "#split data\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate instance of nn, use 64x16x64 autoencoder\n",
    "\n",
    "#create desired architecture\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},  # encoder\n",
    "    {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"sigmoid\"}  # decoder\n",
    "]\n",
    "#experiment with different activation functions - TO DO\n",
    "\n",
    "#initialize nn\n",
    "autoencoder = nn(\n",
    "    nn_arch=nn_arch,\n",
    "    lr=0.001,           # learning rate\n",
    "    seed=42,            # random seed\n",
    "    batch_size=32,      # batch size\n",
    "    epochs=100,         # number of epochs\n",
    "    loss_function= 'mean_squared_error' # loss function (Mean Squared Error for autoencoders)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes in forward pass, before calling single forward pass:\n",
      "X shape: (32, 64) # (batch_size, feature_num)\n",
      "current A_prev, right before single pass call 1\n",
      "A_prev shape: (32, 64) # (batch_size, feature_num)\n",
      "shapes in single forward pass, W_curr, A_prev, b_curr\n",
      "W_curr shape: (16, 64) # (output_neuron, features/input neuron)\n",
      "A_prev shape: (32, 64) # (batch_size, feature_num/input neuron)\n",
      "b_curr shape: (16, 1) # (output_neuron, )\n",
      "shapes in single forward pass, after Z_curr calc, before activation func, Z_curr\n",
      "Z_curr shape: (32, 16) # (batch_size, output_neuron)\n",
      "shapes in single forward pass, after Z_curr calc, and activation func, Z_curr, A_curr\n",
      "Z_curr shape: (32, 16) # (batch_size, output_neuron)\n",
      "A_curr shape: (32, 16) # (batch_size, output_neuron)\n",
      "shapes in forward pass, after calling single forward pass, A, Z\n",
      "A shape: (32, 16) # (batch_size, output_neuron)\n",
      "Z shape: (32, 16) # (batch_size, output_neuron)\n",
      "current A_prev, right before single pass call 2\n",
      "A_prev shape: (32, 16) # (batch_size, feature_num)\n",
      "shapes in single forward pass, W_curr, A_prev, b_curr\n",
      "W_curr shape: (64, 16) # (output_neuron, features/input neuron)\n",
      "A_prev shape: (32, 16) # (batch_size, feature_num/input neuron)\n",
      "b_curr shape: (64, 1) # (output_neuron, )\n",
      "shapes in single forward pass, after Z_curr calc, before activation func, Z_curr\n",
      "Z_curr shape: (32, 64) # (batch_size, output_neuron)\n",
      "shapes in single forward pass, after Z_curr calc, and activation func, Z_curr, A_curr\n",
      "Z_curr shape: (32, 64) # (batch_size, output_neuron)\n",
      "A_curr shape: (32, 64) # (batch_size, output_neuron)\n",
      "shapes in forward pass, after calling single forward pass, A, Z\n",
      "A shape: (32, 64) # (batch_size, output_neuron)\n",
      "Z shape: (32, 64) # (batch_size, output_neuron)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,64) (32,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#train autoencoder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss_train, loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Biophysics PhD/Biophysics Winter 25/Algorithms/final-nn/nn/nn.py:496\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    493\u001b[0m y_hat, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_train_batch)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m#call backprop\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m grad_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m#update params\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_params(grad_dict)\n",
      "File \u001b[0;32m~/Documents/Biophysics PhD/Biophysics Winter 25/Algorithms/final-nn/nn/nn.py:331\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, y, y_hat, cache)\u001b[0m\n\u001b[1;32m    329\u001b[0m     dA_curr\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_binary_cross_entropy_backprop(y, y_hat)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m     dA_curr\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean_squared_error_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss function not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Biophysics PhD/Biophysics Winter 25/Algorithms/final-nn/nn/nn.py:771\u001b[0m, in \u001b[0;36mNeuralNetwork._mean_squared_error_backprop\u001b[0;34m(self, y, y_hat)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03mMean square error loss derivative for backprop.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m        partial derivative of loss with respect to A matrix.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;66;03m#m = y_hat.shape[1] # sample number  #switched to y_hat\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m#need m? \u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m#return (2 / m) * ( y_hat - y)\u001b[39;00m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m( \u001b[43my_hat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,64) (32,) "
     ]
    }
   ],
   "source": [
    "#train autoencoder\n",
    "loss_train, loss_val = autoencoder.fit(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the digits dataset through sklearn using `sklearn.datasets.load_digits()`.\n",
    "- Split the data into training and validation sets.\n",
    "- Generate an instance of your NeuralNetwork class with a 64x16x64 autoencoder architecture.\n",
    "- Train your autoencoder on the training data.\n",
    "- Plot your training and validation loss by epoch.\n",
    "- Quantify your average reconstruction error over the validation set.\n",
    "- Explain why you chose the hyperparameter values you did."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
